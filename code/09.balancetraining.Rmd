---
title: "Balance Training Data"
author: "Maggie Church"
date: "October 12, 2024"
updated: "2025-02-24"
output: html_document
---

Balance training data spatiotemporally
Cols added: grid_id and ecoregion

```{r, message=F}
# install.packages("Ckmeans.1d.dp")
# install.packages("tidyterra")

library(tidyverse)
library(sf)
library(here)

source(here("code", "helper_functions", "perform_kmeans.R"))

plots <- st_read(here("data", "allPlots"))
ppr <- st_read(here("data", "boundaries", "PPJV"))
```

Load training data
```{r, message=F}
training <- st_read(here("data", "train_test_data", "unbalanced_training", "training_2wk_200pt.shp"))
```

# set train years w full ppr coverage (we'll create CV blocks for these, later)
```{r}
trainyrs_fullppr <- c("2019", "2021", "2023")
```

## divide PPR into a grid
```{r}
set.seed(123)

# add a grid
grid <- st_make_grid(st_transform(ppr, 3857), cellsize=(90000)) %>% st_transform(4326)
index <- which(lengths(st_intersects(grid, plots)) > 0)
grid_ppr <- grid[index]

# convert the grid to sf
grid_sf <- grid_ppr %>%  
  st_as_sf() %>%
  mutate(grid_id = row_number()) 

# add proportion grid_id within the ppr
intersection <- st_intersection(grid_sf, ppr) 
intersection$grid_area <- st_area(intersection) %>% as.numeric() 
intersection <- intersection %>% mutate(grid_area_prop = grid_area/max(grid_area))
intersection <- intersection %>% st_drop_geometry() %>% select(grid_id, grid_area_prop)

# add grid_id to dataset
training <- training %>% 
  st_join(grid_sf, join = st_intersects) %>% # add grid_id
  left_join(intersection)                    # add grid_area_prop
```

## balance dataset

Class balancing usually works by adding or removing rows (or adjusting weights).

The common situation is that your classes themselves are unbalanced. For that, check out [this documentation](https://mlr-org.com/gallery/basic/2020-03-30-imbalanced-data/index.html).

My goal is to balance my samples spatially across the "grid," within each year. Basically, I don't want to over-train in one region. Remaining data gaps include:

-   W. MT has none 2019
-   2017 is ND-only

```{r}
set.seed(123)

# Define threshold for undersampling and oversampling
balanced_thresh <- 300

# Undersample in high-density cells - thin points to meet n points per type
undersampled <- training %>%
  group_split(type, dataset, grid_id) %>%
  lapply(function(df) {
    n_threshold <- round(unique(df$grid_area_prop) * balanced_thresh)
    if (nrow(df) >= n_threshold) {
      df %>% sample_n(size = n_threshold, replace = FALSE)
    } 
  }) %>%
  bind_rows()

# Oversample in low-density cells - duplicate points to meet n points per type
oversampled <- training %>%
  group_by(type, dataset, grid_id) %>%
  mutate(n_points = n(),
         n_threshold = round(grid_area_prop * balanced_thresh)) %>%
  filter(n_points < n_threshold) %>%  # Filter for low-density groups
  do({
    # Calculate needed points per group
    needed_points <- unique(.$n_threshold) - nrow(.)  
    if (needed_points > 0) {
      # Sample with replacement
      augmented_points <- slice_sample(., n = needed_points, replace = TRUE)  
      # Combine original and oversampled points
      bind_rows(., augmented_points)  
    } 
  }) %>%
  ungroup()

# Combine the undersampled and oversampled datasets
balanced_training <- 
  bind_rows(undersampled, oversampled) %>% 
  # drop the dry points from a grid-year if it didn't have any wet points
  group_by(dataset, grid_id) %>% 
  filter(!is.na(sum(type=="wet")) & sum(type=="wet") > 0) %>% 
  ungroup() 
```

## Create blocks for CV
(2 groups per full year, 1 for each 2017 season)
```{r}
set.seed(123)

# Split by dataset and apply kmeans to each group 
clusters <- balanced_training %>%
  filter(dataset %in% trainyrs_fullppr) %>%
  group_split(dataset) %>%
  lapply(perform_kmeans, k=2) %>% 
  bind_rows() %>% 
  bind_rows(balanced_training %>% filter(!dataset %in% trainyrs_fullppr) %>% mutate(cluster_id=1)) %>% 
  ungroup() %>% 
  mutate(block = paste(cluster_id, "-", dataset))
```

Calculate how many ponds we thin/oversample
```{r, eval=F}

# these are the ponds we'll thin
n_undersampled_pre <- training %>%
  st_drop_geometry() %>%
  filter(type=="wet") %>% 
  group_by(dataset, grid_id) %>% 
  filter(n() >= balanced_thresh*grid_area_prop) %>% 
  nrow()

n_undersampled_post <- undersampled %>% 
  st_drop_geometry() %>%
  filter(type=="wet") %>% 
  nrow()

# these are the ponds we'll oversample
n_oversampled_pre = training %>%
  st_drop_geometry() %>%
  filter(type=="wet") %>% 
  group_by(dataset, grid_id) %>% 
  filter(n() < balanced_thresh*grid_area_prop) %>% 
  nrow()

n_oversampled_post <- oversampled %>% 
  st_drop_geometry() %>%
  filter(type=="wet") %>% 
  nrow()

# total ponds
n_total <- training %>% 
  st_drop_geometry() %>%
  filter(type=="wet") %>% 
  nrow()

n_bal_total <- balanced_training %>% 
  st_drop_geometry() %>%
  filter(type=="wet") %>% 
  nrow()


n_total                                  # n training ponds
n_undersampled_pre - n_undersampled_post # (-) we thin this many --> 207k
n_oversampled_post - n_oversampled_pre   # (+) we oversample this many -> 4500
n_bal_total                              # n balanced training ponds -> 70k
```

# Export
Write balanced training dataset to disk
```{r}
clusters %>% write_csv(here("data", "train_test_data", "balanced_training", "balanced_training_2wk_200pt_bal.csv"))

clusters %>% st_write(here("data", "train_test_data", "balanced_training", "balanced_training_2wk_200pt_bal.shp"), append=F)
```
