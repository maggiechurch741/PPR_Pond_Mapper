---
title: "Balance Training Data"
author: "Maggie Church"
date: "October 12, 2024"
output: html_document
editor_options: 
  chunk_output_type: inline
  markdown: 
    wrap: 72
---


```{r, message=F}
# install.packages("Ckmeans.1d.dp")
# install.packages("tidyterra")

library(sf)
library(tidyverse)
```

Load training data
(training is 2017, 2019, 2020, 2022)
```{r, message=F}
training <- st_read('../data/intermediate/unbalanced_training/training.csv') %>%
  filter(!(red==0 & nir==0)) # drop this 1 row
```

## balance dataset

Class balancing usually works by adding or removing rows (or adjusting weights).

The common situation is that your classes themselves are unbalanced. For that, check out [this documentation](https://mlr-org.com/gallery/basic/2020-03-30-imbalanced-data/index.html).

My goal is to balance my samples spatially across the "grid," within each year. Basically, I don't want to overtain in one region. Remaining data gaps include:

-   W. MT has none 2019

-   IA has none in 2020

-   2017 is ND-only

-   2020 has none \<2 acres


Equalize dry and wet points. There are generally more dry points than wet, or at least a close amount 

```{r}
med <- training %>%
  st_drop_geometry() %>% 
  filter(type=="wet") %>% 
  group_by(dataset, grid_id) %>% 
  #filter(n() >= 30) %>% 
  count() %>% 
  ungroup() %>%
  summarize(median(n))

# print median
med
```

```{r, eval=F}
# histogram of spatial balance
training %>% 
  st_drop_geometry() %>% # speeds things up
  filter(type=="wet") %>% 
  group_by(grid_id, dataset) %>%
  count() %>% 
  ungroup() %>% 
  #filter(n>=30) %>% 
  filter(n < 2000) %>% # drop this 1 bc it's messing with the histogram
  pull() %>% 
  hist(breaks=40, main = "Histogram of training points per grid box-year", sub="does not include 1 box with >2000 wet points (for viewability)") 

abline(v=med, lty="dashed") 
```

```{r}
temp <- st_join(grid_sf, training, join = st_intersects) %>% 
  filter(type=="wet") %>% 
  group_by(grid_id.x, dataset) %>% 
  count() 

#mapview(temp %>% filter(n<30), col.regions="gray") + 
  mapview(temp %>% filter(n < 50),  col.regions="red") +
  mapview(temp %>% filter(n >= 50),  col.regions="blue") 
```

Highest density of wet points in a grid square is 3622 Lowest density is 1. After dropping boxes with fewer than 30 points, median density is 150

```{r}
set.seed(123)

# Define threshold for undersampling and oversampling
# balanced_points <- round(pull(med))
balanced_points <- 100

# Undersample in high-density cells - thin points to meet [balanced_points] per type
undersampled <- training %>%
  group_by(type, dataset, grid_id) %>% 
  filter(n() >= balanced_points) %>%
  sample_n(balanced_points) 

# Oversample in low-density cells - duplicate points to meet [balanced_points] per type
oversampled <- training %>%
  group_by(type, dataset, grid_id) %>% 
  mutate(n_points = n()) %>% 
  filter(n_points < balanced_points) %>%  
  do({
    # Get the number of needed points
    needed_points <- balanced_points - unique(.$n_points)  
    # Sample new points with replacement to augment points
    augmented_points <- slice_sample(., n = needed_points, replace = TRUE)  
    # Combine original and augmented points
    bind_rows(., augmented_points)  
  }) %>% 
  ungroup() 

# hmm there are just a couple cases where n_dry <30 but n_wet >30 -- would wanna keep these.

# Combine the undersampled, oversampled, and already balanced datasets
balanced_training <- 
  bind_rows(undersampled, oversampled) %>% 
  # drop the dry points from a grid-year if it didn't have any wet points
  group_by(dataset, grid_id) %>% 
  filter(!is.na(sum(type=="wet")) & sum(type=="wet") > 0) %>% 
  ungroup() 
```

```{r, eval=F}

n_undersampled_pre <- training %>%
  st_drop_geometry() %>%
  group_by(type, dataset, grid_id) %>% 
  filter(n() >= balanced_points) %>% 
  filter(type=="wet") %>% 
  nrow()

n_undersampled_post <- undersampled %>% 
  st_drop_geometry() %>%
  filter(type=="wet") %>% 
  nrow()

# these are the ones we'll oversample
n_oversampled_pre = training %>%
  st_drop_geometry() %>%
  group_by(type, dataset, grid_id) %>% 
  mutate(n_points = n()) %>% 
  filter(if_else(type == "wet", n_points >= 30, TRUE)) %>%  # Drop grid-year if fewer than 30 wet points
  filter(n_points < balanced_points) %>% 
  filter(type=="wet") %>% 
  nrow()

n_oversampled_post <- oversampled %>% 
  st_drop_geometry() %>%
  filter(type=="wet") %>% 
  nrow()

n_total <- training %>% 
  st_drop_geometry() %>%
  filter(type=="wet") %>% 
  nrow()

n_bal_total <- balanced_training %>% 
  st_drop_geometry() %>%
  filter(type=="wet") %>% 
  nrow()

n_sm20 <- bind_rows(oversampled, undersampled) %>% 
  st_drop_geometry() %>%
  filter(type=="wet" & area_cr < 2 & dataset == "2020") %>% 
  nrow()

n_total 
n_undersampled_pre - n_undersampled_post # (-) we thin this many
n_oversampled_post - n_oversampled_pre   # (+) we oversample this many
n_sm20                                   # (-)
n_bal_total
```
We start with 83k ponds
At balancing point = 66, we thin 62k wet points, oversample 12k, drop 7k small 2020 ponds, and end up with 31k
At balancing point = 100, we thin 54k wet points, oversample 20k, drop 11k small 2020 ponds, and end up with 47k

```{r}
set.seed(123)

# let's dump small 2020 ponds bc we don't trust them
balanced_training_nosmall20 <- balanced_training %>% filter(!(area_cr < 2 & dataset == "2020") | type=="dry")

# reduce dry points to match wet
dry_2020 <- balanced_training_nosmall20 %>%
  filter(dataset=="2020") %>% 
  group_by(grid_id, dataset) %>%
  mutate(n_wet = sum(type == "wet")) %>%
  filter(type=="dry") %>% 
  do({
    needed_points <- unique(.$n_wet)
    augmented_points <- slice_sample(., n = needed_points)
  }) 

balanced_training_final <- balanced_training_nosmall20 %>% 
  filter(!(dataset=="2020" & type == "dry")) %>% 
  bind_rows(dry_2020)
```

```{r}
set.seed(123)

######################
# Function to perform k-means on each dataset's coordinates
perform_kmeans <- function(data) {
  coords <- st_coordinates(data)  
  kmeans_result <- kmeans(coords, centers = 5, algorithm = "Lloyd", iter.max = 60)
  data$cluster <- kmeans_result$cluster  
  return(data)  
}

# Split by dataset and apply kmeans to each group
clusters <- balanced_training_final %>%
  filter(dataset %in% c("2019", "2020", "2022")) %>%
  group_split(dataset) %>%
  lapply(perform_kmeans) %>% 
  bind_rows() %>% 
  bind_rows(balanced_training_final %>% filter(!dataset %in% c("2019", "2020", "2022")) %>% mutate(cluster=1)) %>% 
  rename(cluster_id=cluster) %>% 
  ungroup() %>% 
  mutate(block = paste(cluster_id, "-", dataset))
```

Write file to disk
```{r}
clusters %>% st_write("../data/intermediate/balanced_training/balanced_training_b100.shp", append=F)

clusters %>% write_csv("../data/intermediate/balanced_training/balanced_training_b100.csv")
```
